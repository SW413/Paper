\chapter{Test of Language}
\label{cha:test_of_language}
In order to evaluate the language there is performed a test. 
As performance is in focus in the project the project group has chosen to only test the language by comparing its performance when using the \gls{gpu} compared to when using the \gls{cpu}.
Priviously handwritten C code for the \gls{cpu} has been compared to handwritten CUDA C code running on the \gls{gpu}, see \myref{subsec:gpuvscpuv1}.

The results of this test can within reason be expected to have a similar output, meaning that at low data sizes the \gls{cpu} is faster than the \gls{gpu}, however as the size of the data gets better the \gls{gpu} is faster.
The reason the \gls{gpu} is slow for low data sizes is that memory needs to be allocated on the \gls{gpu}, the input data must be moved to the \gls{gpu}, the calculation must be done, and then the data can be transfered back to the \gls{cpu}. 
From which it can be further processed, written to a file, output to stdout etc. 
This in total means that a relatively high amount of time will be used not doing calculations. 

It is to be noted that neither the code for the \gls{cpu} nor the \gls{gpu} is optimized by anything other than the gcc-compiler, as explained in \todo{ref til makefile afsnit}. 
This means that the code could be optimized, for example by loop-unrolling, multi-core use with threading, SIMD instructions etc. for the \gls{cpu}. \todo{Evt. kunne vi fors√∏ge os med forskellige optimeringer}
And by varying the size of the work group, offline kernel compilation, advanced memory use i.e. local memory, for the \gls{gpu}.  

\section{The test}
The test performed will be a matrix multiplication, in gamble this is used with the syntax \texttt{matrixA * matrixB}. 
This compiles to a kernel which is invoked for each index in the result matrix, the kernel each calculate just one index of the result, and has an inner loop of the size of the columns of the first matrix, which is equal to the rows of the second matrix. 

With square matrices, of size $n$ as input the amount of calculations is $2*n^3 = O(n^3)$ operations. 
And there are $3*n^2 = O(n^2)$ numbers in matrices which will be operated on, $2/3$ will be read from and $1/3$ will be written to. 