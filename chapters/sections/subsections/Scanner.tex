\subsection*{Scanner}
The first stage of syntax analysis is the lexical analysis, which is often done in something called a scanner but can also be called the lexer.
The primary function of a scanner is to transform a sequence of characters into a sequence of tokens.
The purpose of these tokens is to transform the raw source code into some tokens which can be meaningful interpreted in the coming phases of the compiler.
  
This is done by scanning every non-whitespace character, and grouping the characters into these tokens.
The scanner makes sure that the source code adheres to the grammar rules provided by the \acrshort{cfg}.
An example of this, would be that you could use the notation .1 or 0.1 for a decimal number, both being turned into identical tokens by the scanner.

Some examples of our lexical rules for \gls{gamble} can be seen on \myref{lst:token}.
The definition of an integer number on line 3 states that an integer is either a zero or an optional negative sign followed by a single digit from one to nine followed by zero or more numbers from zero to nine.
It is necessary to clearly define tokens for the lexer to read in order to read source code correctly. \citep{Crafting_book}

\begin{lstlisting}[caption=Example of our lexer rules for \gls{gamble},frame=tlrb,label={lst:token}]
// Integers
INT: 'int' | 'int16' | 'int32' | 'int64' ; // Integers
INTNUM: '0' | SIGN? [1-9][0-9]* ;

// Matrices and vectors
MATRIX: 'matrix' ;
VECTOR: 'vector' ;

// Whitespace and comments
WS: [ \t ]+ -> skip;
NL: [ \r \n | \n ] -> skip;

COMMENT
    :   '/*' .*? '*/' -> skip
    ;

LINE_COMMENT
    :   '//' ~[\r\n]* -> skip
    ;
\end{lstlisting} 