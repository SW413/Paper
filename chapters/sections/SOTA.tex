\section{Existing solutions} % (fold)
\label{sec:state_of_the_art}
In this section different approaches to \acrshort{gpgpu} using existing programming languages and libraries will be presented.
Generally there exists two ways of utilising the \acrshort{gpu} at a low level for \acrshort{gpgpu}, these are OpenCL by The Khronos Group and CUDA by NVIDIA.
These two are in many ways similar but CUDA is only developed for NVIDIA \acrshortpl{gpu}, while OpenCL is supported by several processing units including but not limitied to \acrshortpl{cpu} and \acrshortpl{gpu}.
Each language and library listed below use either one or both of these technologies.
Every language and library described in this section can be found on \myref{tbl:sota} for an overview of their comparisons.
For this analysis the level of abstraction of a language or library is based upon how much control the programmer of the code has of the computer's resources, i.e. if the programmer needs a line of code to allocate memory for each of the computations in the code, that results in low abstraction.
      
\subsection{Libraries} 
There exists libraries for programming languages in order to utilise the \acrshort{gpu} for computations by binding either to OpenCL or CUDA.
Generally the libraries used for \acrshort{gpu} work often requires a lot of boilerplate and has a low level of abstraction.
Boilerplate is the pieces of code which will have to be written with little to no alteration, in many different places of the code.
An example could be when making a call to a \acrshort{gpu}, the boilerplate code might be the code which handles this communication.
As an example we will look at C, Java and R, and some of their \acrshort{gpu} Libraries.

Jcuda is a library for Java which support the use of CUDA, it has a lot of boilerplate, and low/medium abstraction level. \citep{Java_library}
Jcuda requires many imports and the user needs to allocate a memory block for each element which causes the boilerplate code and the level of abstraction given. \citep{Java_malloc}
C has libraries such as CUDA C, OpenCL C and others.
These C libraries have the same design as with the Java library as one needs to allocate memory for almost everything, and there is a lot of redundancy which creates a lot of boilerplate. 
The abstraction level is therefore deemed low as one must keep in mind what is where and what can be done with each specific element. \citep{C_CUDA} 
There are also different libraries for the R language, one of which is called rpud.
It provides many functions from the R language, which can be performed on the \acrshort{gpu}, and it is based on CUDA.
We deem that it has a high level of abstractions, since it is just function calls, without any memory allocation or direct \acrshort{gpu} calls from the source code. \citep{Rcuda} 

\subsection{Theano (Python)}
Theano is a Python library that allows one to define and evaluate mathematical expressions involving multi-dimensional arrays, while utilising the \acrshort{gpu}.
The library has two ways of using the \acrshort{gpu}; one using CUDA, and one using OpenCL.
The OpenCL implementation does not support all the options available in the library due to an incomplete port of an old back-end.
While Theano supports CUDA and OpenCL, there is quite a bit of boilerplate and one must write different code in order to use either.

Theano does not require that the programmer explicitly allocates memory for arrays.
Theano has a high level of abstraction since one has to declare if the \acrshort{gpu} should be used and can only operate on single precision floats of 32 bits.
On the other hand Theano does increase its performance by replacing methods with a \acrshort{gpu} versions of the same methods to create transparency. \citep{Theano,Theano_GPU,bergstratheano, LEGB}

\subsection{MATLAB}
MATLAB is a high-level mathematical programming language with an interactive environment.
MATLAB supports the use of parallel computations in the form of using either a \acrshort{gpu} or cloud computing.
It natively supports the use of CUDA enabled NVIDIA \acrshortpl{gpu} for its parallel computations on \acrshortpl{gpu}, but OpenCL extensions do exist such that it becomes possible to use other devices.
The user of MATLAB must declare when the \acrshort{gpu} must be used and can also allocate memory for these tasks, or choose that MATLAB can do so automatically.
Declaring the memory produces boilerplate since it needs to be done for each element that one wants to compute. \citep{MATLAB_backend,MATLAB_benchmark}

Using the interactive environment provided by MATLAB there are built in tools for parallel computations on \acrshortpl{gpu}.
These tools provide a higher level of abstraction such as parallel for-loops (parfor) and special array types for distributed processing.
For \acrshort{gpu} computing MATLAB simplify parallel code development by abstracting away the complexity of managing computations and data. \citep{MATLAB_parallel}
Depending on whether the programmer wants to fully utilise the \acrshort{gpu} or not the level of abstraction differs from low to high.

\subsection{Julia}
Julia is a high-level, high-performance dynamic programming language for technical computing.
Julia is designed for parallelism and cloud computing; making it efficient and easy to use for these tasks.
Julia has a high level of abstraction because the user only needs a single keyword (\@parallel) for it to do the calculation in parallel.\todo{ligesom Theano men ikke samme abstraction level?}
The code is therefore looking clean without any boilerplate and is easy to read.
Julia can use either OpenCL or CUDA as back-end making it very compatible and easy to use on different systems and devices.
Julia uses similar scoping rules as MATLAB. \citep{Julia_Git, Julia_scope,Julia}

\input{figures/MathGPULanguages.tex} 

\subsection{Conclusion}  

The different languages mentioned in this section all have some way of communicating with the \acrshort{gpu} using both CUDA and OpenCL, which can also be seen on \myref{tbl:sota}.
They all require the user to specify when to use the \acrshort{gpu} instead of the \acrshort{cpu}, and the level of abstraction varies, where some require a lot of control of the programmer, and others require specific function calls to \acrshort{gpu} functions.

This explicit processor targeting can be inconvenient and if used incorrectly can be inefficient, and it requires that the programmer write more code than the calculations themselves.
On the other hand, it gives the programmer more control of where the code is executed.
This aspect requires a deeper knowledge of processor architecture, and we deem that greater abstraction is more important than the control of processor targeting.

They are all usable for performing calculations on the \acrshort{gpu}, except the libraries for R, which only provides the programmer with specific R functions to be computed on the \acrshort{gpu}. 
For scientific computing, especially fields that use linear algebra, these are all viable options.

The control of memory on the \acrshort{cpu} and the \acrshort{gpu} is different, so allocating memory is more tedious when you transfer the calculations to the \acrshort{gpu}.
Therefore languages that require the programmer to allocate memory themselves, takes away focus from the computations that need to be done.
We deem that explicitly allocating memory is unnecessary, and can instead be done implicitly.

The next section will discuss the analysis which has been done so far and will also construct a problem statement which the remaining portion of the paper will answer.