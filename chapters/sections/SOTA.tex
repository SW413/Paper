\section{Existing solutions} % (fold)
\label{sec:state_of_the_art}
In this section different approaches to \acrshort{gpgpu} using existing programming languages and libraries will be presented.
Each language and library will be running with either the OpenCL or the CUDA framework.
Every language and library described in this section can be found on \myref{tbl:sota} for an overview of their comparisons.
      
\subsection{Libraries} 
There exists libraries for programming languages in order to utilise the \acrshort{gpu} for computations by binding either to OpenCL or CUDA.
Generally the libraries used for \acrshort{gpu} work often requires a lot of boilerplate and has what we deem to be a low level of abstraction.
The level of abstraction is based on how much control the programmer of the code has of the computer's resources.
I.e. if the programmer needs a line of code to allocate memory for each of the computations in the code, that results in low abstraction.
Boilerplate is the pieces of code which will have to be written with little to no alteration, in many different places of the code.
An example could be when making a call to a \acrshort{gpu}, the boilerplate might be the code which handles this communication.
As an example we will look at C, Java and R, and some of their \acrshort{gpu} Libraries.
Jcuda is a library for Java which support the use of CUDA, it has a lot of boilerplate, and low/medium abstraction level. \citep{Java_library}
Jcuda requires many imports and the user needs to allocate a memory block for each element which causes the boilerplate code and the level of abstraction given. \citep{Java_malloc}
C has libraries such as CUDA C, OpenCL C and others.
These C libraries have the same problem as with the Java library as one needs to allocate memory for almost everything, and there is a lot of redundancy which creates a lot of boilerplate.
The abstraction level is therefore deemed low as one must keep in mind what is where and what can be done with each specific element. \citep{C_CUDA} 
There is also different libraries for the R language, one of which is called rpud.
It provides many functions from the R language, which can be performed on the \acrshort{gpu}, and it is based on CUDA.
We deem that it has a high level of abstractions, since it is just function calls, without any memory allocation or direct \acrshort{gpu} calls from the source code. \citep{Rcuda} 

\subsection{Theano (Python)}
Theano is a Python library that allows one to define, optimise, and evaluate mathematical expressions involving multi-dimensional arrays, while utilising the \acrshort{gpu}.
The library has two ways of using the \acrshort{gpu}; one with CUDA as back-end, and the other that should support any OpenCL compatible device as well NVIDIA cards.
The OpenCL implementation however does not support all the options available in the library due to an incomplete port of an old back-end.
While Theano supports CUDA and OpenCL, there is quite a bit of boilerplate and one must write different code in order to use either.	
The scoping rules for Python which Theano uses are quite special where the compiler looks for a variable in four different scopes.
These scoping rules seems to be exclusive to the Python language.
This can cause some problems for new programmers of the language, because these scoping rules are special.
 Theano does not require that the programmer allocates memory for arrays.
We deem that Theano has a medium level of abstraction since one has to declare if the \acrshort{gpu} should be used and can only operate on single precision floats of 32 bits.
But on the other hand Theano does optimise the code by replacing methods with a \acrshort{gpu} versions of the same methods to create transparency. \citep{Theano,Theano_GPU,bergstratheano, LEGB}

\subsection{MATLAB}
MATLAB is a high-level mathematical programming language with an interactive environment.
MATLAB supports the use of parallel computations in the form of using either a \acrshort{gpu} or cloud computing.
It only natively supports the use of CUDA enabled NVIDIA \acrshortpl{gpu} for its parallel computations on \acrshortpl{gpu}, but OpenCL extensions do exist such that it becomes possible to use other devices.
MATLAB uses static scoping  with rules very similar to programming languages like.
The user of MATLAB much declare when the \acrshort{gpu} must be used and also allocate memory for these tasks.
Declaring the memory gives a lot of boilerplate since it needs to be done for each element that one wants to compute. \citep{MATLAB_backend,MATLAB_benchmark}

Using the interactive environment provided by MATLAB there are built in tools for parallel computations on \acrshortpl{gpu}.
These tools provide a higher level of abstraction such as parallel for-loops (parfor) and special array types for distributed processing.
For \acrshort{gpu} computing MATLAB simplify parallel code development by abstracting away the complexity of managing computations and data. \citep{MATLAB_parallel}

\subsection{Julia}
Julia is a high-level, high-performance dynamic programming language for technical computing, with syntax familiar to users of other technical computing environments.
Julia is designed for parallelism and cloud computing; making it efficient and easy to use for these tasks.
Julia has a high level of abstraction because the user only needs a single keyword (\@parallel) for it to do the calculation in parallel.
The code is therefore looking clean without any boilerplate and is easy to read.
Julia uses both OpenCL and CUDA as back-end making it very compatible and easy to use on different systems and devices.
Julia uses similar scoping rules as MATLAB.\citep{Julia_Git,Julia, Julia_scope}

\input{figures/MathGPULanguages.tex} 

\subsection{Conclusion}  

The different languages mentioned in this section all have some way of communicating with the \acrshort{gpu} using both CUDA and OpenCL, which also can be seen on \myref{tbl:sota} .
They all require the user to specify when to use the \acrshort{gpu} instead of the CPU, and the level of abstraction varies, where some require a lot of control of the programmer, and others require specific function calls to \acrshort{gpu} functions.

This explicit processor targeting seems inconvenient, and it takes away focus from the calculations done in the program.
On the other hand, it gives the programmer more control of where the code is executed.
This aspect requires a deeper knowledge of processor architecture, and we deem that greater abstraction is more important than the control of processor targeting.

They are all usable for general purpose programming, except the libraries for R, which only provides the programmer with specific R functions to be computed on the \acrshort{gpu}.
For scientific computing, especially oriented towards matrices and vector calculations such as linear algebra uses, these are all viable options.

The control of memory on the CPU and the \acrshort{gpu} is different, so allocating memory is more difficult when you transfer the calculations to the \acrshort{gpu}.
Therefore languages that require the programmer to allocate memory themselves, takes away focus from the computations that need to be done.
We deem that allocating memory for arrays and matrices is unnecessary.
                     