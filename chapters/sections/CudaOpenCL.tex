\section{Parallel computing platform}
In \myref{CUDAvsOpenCL} it was decided whether to use the parallel computing platform developed by NVIDIA, CUDA, or the one developed by The Khronos Group, \gls{opencl}.
The choice fell on \gls{opencl} due to its cross platform capabilities.
In this section we will discuss how this choice affected the project, whether or not it was the best choice and the perks as well as the flaws encountered whilst using \gls{opencl}.

The significant factor that made the choice fall upon \gls{opencl} was its cross platform abilities; once the \gls{opencl} C code generation started complications in its cross platform abilities was met.
While indeed \gls{opencl} can run across multiple platform, this feature is not without its faults.
This especially became evident as different \acrshort{gpu}s support different versions of \gls{opencl}. thus having different features implemented.
It became even more clear that while \gls{opencl} is a standard for cross platform, it does not mean that the same code will work across hardware, this was experienced by the project group when NVIDIAs hardware became troublesome and produced errors, for code which runs perfectly on AMD hardware, while running the same operating system, Ubuntu 14.04 LTS.
Furthermore, as mentioned in \myref{subsec:runtime} different \acrshort{gpu}s also perform different on the same code, even if the \acrshort{gpu}s have near identical specifications.
This as a result means that to achieve the most benefit from each individual platform, an implementation for each platform should be available; having to alter the implementation of kernels to each platform makes the cross platform feature less useful than initially thought.
While the project group have not made any specifications towards a specific platform, if done so, it may be worthwhile to consider using \gls{cuda} for an NVIDIA implementation, and \gls{opencl} for other \acrshort{gpu}s or processing units in general.

%This may be the best place for this, but i do believe it necessary to be in the paper
Another problem the project group encountered quite early was that Windows is not the most well suited choice for our development environment.
Using Visual Studio the project group was able to run \gls{opencl} C example programs, thus proving that the code worked on the Windows operating system, however doing so through a terminal and the GCC compiler proved to be quite difficult. 
In an attempt to solve this issue two things also became evident to the project group, since configuring the development environment using Windows proved difficult.
Most documentation available was aimed at OS X and Linux operating systems, as a result the project group proceeded development using these operating systems rather than Windows.

Another thing realised by the project group while learning about \gls{opencl} was that help and web resources for \gls{opencl} was represented by a small margin of what was available for CUDA.
As a result this led to many trial and error tests to figure out the workings of \gls{opencl} and debugging of encountered errors, where as if \gls{cuda} had been the parallel computing platform used, it may have been easier to acquire help through web resources.

\myref{image:OpenCLOptCompare} shows a significant performance difference between the native implementations of \gls{cuda} and \gls{opencl} is shown.
While \gls{opencl} and \gls{cuda} can reach a somewhat similar performance, the naive implementation in \gls{opencl} with no attempt to increase runtime efficiency is about seven times slower than the naive implementation in \gls{cuda} due to the \gls{cuda} compiler altering the \acrshort{ptx} code to increase runtime efficiency.
As a result, to gain performance with the limited experience with \acrshort{gpu} computing in the project group, \gls{cuda} may have been the better choice, even if the same performance is obtainable in \gls{opencl}.

While \gls{opencl} does work cross platform, an ideal implementation would still be different per \acrshort{gpu}, with \gls{cuda} definitely being the better choice for NVIDIA hardware.
Furthermore as has been mentioned, the \gls{opencl} C compiler requires the programmers to perform runtime efficiency improvements to achieve the performance native to the \gls{cuda} compiler alternative.
This would have made using \gls{cuda} more beneficial in terms of performance, as attempting to do so in \gls{opencl} C through compiler optimisations requires an intricate knowledge of compiler creation and \acrshort{gpu} computing.

Should \gls{gamble} be further developed or a similar language developed, the opinion of the project group is to first develop an implementation for a target platform, if NVIDIA is the target use \gls{cuda} and if AMD or Intel are the targets use \gls{opencl}.
As for \gls{gamble} it may have made it easier to target NVIDIA \acrshort{gpu}s by using \gls{cuda} and ignore AMD and Intel \acrshort{gpu}s. 
This could possibly have led to greater performance from \gls{gamble} at the cost of only being of use to users with NVIDIA hardware.
With the required knowledge however, one should choose the parallel computing platform, depending on what hardware one expects its users to use.
